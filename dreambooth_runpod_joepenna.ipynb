{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b971cc0",
   "metadata": {
    "id": "7b971cc0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24985b2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ model.ckpt successfully downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download the 1.4 sd model\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "downloaded_model_path = hf_hub_download(\n",
    " repo_id=\"stabilityai/stable-diffusion-2\",\n",
    " filename=\"768-v-ema.ckpt\",\n",
    " use_auth_token=True\n",
    ")\n",
    "\n",
    "# Move the sd-v1-4.ckpt to the root of this directory as \"model.ckpt\"\n",
    "actual_locations_of_model_blob = !readlink -f {downloaded_model_path}\n",
    "!mv {actual_locations_of_model_blob[-1]} sd2.ckpt\n",
    "clear_output()\n",
    "print(\"✅ model.ckpt successfully downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67f9ff0c-b529-4c7c-8e26-8388d70a5d91",
   "metadata": {
    "id": "67f9ff0c-b529-4c7c-8e26-8388d70a5d91",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_format_ts():\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    return datetime.fromtimestamp(int(time.time())).strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def get_img_urls(file_name):\n",
    "    urls = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        urls = [r.replace(\"\\n\", \"\") for r in lines]\n",
    "    print(urls)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def generate_training_images(urls, save_path):\n",
    "    import os\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    from PIL import Image\n",
    "\n",
    "\n",
    "    def image_grid(imgs, rows, cols):\n",
    "     assert len(imgs) == rows*cols\n",
    "\n",
    "     w, h = imgs[0].size\n",
    "     grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "     grid_w, grid_h = grid.size\n",
    "\n",
    "     for i, img in enumerate(imgs):\n",
    "      grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "     return grid\n",
    "\n",
    "    def download_image(url):\n",
    "     try:\n",
    "      response = requests.get(url)\n",
    "     except:\n",
    "      return None\n",
    "     return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    images = list(filter(None,[download_image(url) for url in urls]))\n",
    "    save_path = \"./training_images/{}\".format(save_path)\n",
    "    if not os.path.exists(save_path):\n",
    "     os.mkdir(save_path)\n",
    "    [image.save(f\"{save_path}/{i}.png\", format=\"png\") for i, image in enumerate(images)]\n",
    "    image_grid(images, 1, len(images))\n",
    "\n",
    "\n",
    "def generate_reg_images(class_token, resume_model=\"v1-5.ckpt\", self_generated_files_count=200):\n",
    "    !rm -rf ./outputs\n",
    "    !python scripts/stable_txt2img.py \\\n",
    "     --seed 10 \\\n",
    "     --ddim_eta 0.0 \\\n",
    "     --n_samples 1 \\\n",
    "     --n_iter {self_generated_files_count} \\\n",
    "     --scale 10.0 \\\n",
    "     --ddim_steps 50 \\\n",
    "     --ckpt resume_model/{resume_model} \\\n",
    "     --prompt {class_token}\n",
    "\n",
    "    !mkdir -p regularization_images/{class_token}\n",
    "    !mv outputs/txt2img-samples/*.png regularization_images/{class_token}\n",
    "\n",
    "def train_concept(\n",
    "    class_token, \n",
    "    prompt_token, \n",
    "    resume_model=\"v1-5.ckpt\", \n",
    "    is_gen_training_images=False, \n",
    "    is_gen_reg_images=False, \n",
    "    reg_count=1000, \n",
    "    training_image_path=None,\n",
    "    max_training_steps=2000):\n",
    "    # write logs\n",
    "    with open(\"./logs/training_logs.txt\", \"a+\") as f:\n",
    "        content = \"ts: {}, from: {}, to: {}\\n\".format(get_format_ts(), resume_model.replace(\".ckpt\", \"\"), prompt_token)\n",
    "        f.writelines(content)\n",
    "        f.close()\n",
    "    if is_gen_training_images:\n",
    "        training_image_path = prompt_token + \"_\" + get_format_ts()\n",
    "        urls = get_img_urls(\"./img_urls/{}.txt\".format(prompt_token))\n",
    "        generate_training_images(urls, training_image_path)\n",
    "    else:\n",
    "        assert training_image_path is not None\n",
    "    \n",
    "    if is_gen_reg_images:\n",
    "        self_generated_files_count = reg_count\n",
    "        generate_reg_images(class_token, resume_model, self_generated_files_count=self_generated_files_count)\n",
    "    \n",
    "    project_name = prompt_token\n",
    "    \n",
    "    # MAX STEPS\n",
    "    # Match class_word to the category of the regularization images you chose above.\n",
    "    class_word = class_token # typical uses are \"man\", \"person\", \"woman\"\n",
    "    # This is the unique token you are incorporating into the stable diffusion model.\n",
    "    token = prompt_token\n",
    "\n",
    "    reg_data_root = \"./regularization_images/\" + class_token\n",
    "\n",
    "    !rm -rf training_images/.ipynb_checkpoints\n",
    "    !python \"main.py\" \\\n",
    "     --base configs/stable-diffusion/v1-finetune_unfrozen-mu.yaml \\\n",
    "     -t \\\n",
    "     --reg_data_root \"{reg_data_root}\" \\\n",
    "     -n \"{project_name}\" \\\n",
    "     --gpus 0, \\\n",
    "     --data_root \"./training_images/{training_image_path}\" \\\n",
    "     --max_training_steps {max_training_steps} \\\n",
    "     --class_word \"{class_word}\" \\\n",
    "     --token \"{token}\" \\\n",
    "     --no-test \\\n",
    "     --actual_resume \"./resume_model/{resume_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ae6080b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_MEMORY_EFFICIENT_ATTENTION=1\n",
      "Global seed set to 23\n",
      "Running on GPUs 0,\n",
      "{'model': {'base_learning_rate': 1e-06, 'target': 'ldm.models.diffusion.ddpm.LatentDiffusion', 'params': {'reg_weight': 1.0, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'image', 'cond_stage_key': 'caption', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': True, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'embedding_reg_weight': 0.0, 'unfreeze_model': True, 'model_lr': 1e-06, 'personalization_config': {'target': 'ldm.modules.embedding_manager.EmbeddingManager', 'params': {'placeholder_strings': ['*'], 'initializer_words': ['sculpture'], 'per_image_tokens': False, 'num_vectors_per_token': 1, 'progressive_words': False}}, 'unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_spatial_transformer': True, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False}}, 'aux_unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_spatial_transformer': True, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False, 'use_fp16': True}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 512, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'ldm.modules.encoders.modules.FrozenCLIPEmbedder'}}}, 'data': {'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 1, 'num_workers': 1, 'wrap': False, 'train': {'target': 'ldm.data.personalized.PersonalizedBase', 'params': {'size': 512, 'set': 'train', 'per_image_tokens': False, 'repeats': 100, 'coarse_class_text': 'style', 'data_root': './training_images/test_20221126-044925', 'placeholder_token': 'test', 'token_only': False}}, 'reg': {'target': 'ldm.data.personalized.PersonalizedBase', 'params': {'size': 512, 'set': 'train', 'reg': True, 'per_image_tokens': False, 'repeats': 10, 'data_root': './regularization_images/style', 'coarse_class_text': 'style', 'placeholder_token': 'test'}}, 'validation': {'target': 'ldm.data.personalized.PersonalizedBase', 'params': {'size': 512, 'set': 'val', 'per_image_tokens': False, 'repeats': 10, 'coarse_class_text': 'style', 'placeholder_token': 'test', 'data_root': './training_images/test_20221126-044925'}}}}}\n",
      "Loading model from ./resume_model/model.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    }
   ],
   "source": [
    "%env USE_MEMORY_EFFICIENT_ATTENTION=1\n",
    "# 定义新变量\n",
    "def get_format_ts():\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    return datetime.fromtimestamp(int(time.time())).strftime('%Y%m%d-%H%M%S')\n",
    "class_token = \"style\"  # 这个变量是要训练的内容的类目，要用具有实际含义的词，如要训练一种特殊风格的椅子，就用chair, 训练特殊风格的人物，就用person, etc， 如果是一类风格的话，就用style\n",
    "prompt_token = \"test\" # 这个变量要用模型当中没有出现过的词语\n",
    "resume_model = \"model.ckpt\"  # 注意，这里是模型训练的启动点，意思是模型基于已有的哪个模型训练，在训练前问下良伟\n",
    "# resume_model = \"model.ckpt\"\n",
    "\n",
    "is_gen_training_images = False\n",
    "is_gen_reg_images = False\n",
    "reg_count = 4\n",
    "training_image_path = \"test_20221126-044925\"\n",
    "max_training_steps=100\n",
    "train_concept(class_token, prompt_token, resume_model, is_gen_training_images, is_gen_reg_images, reg_count, training_image_path,max_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc7605",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}